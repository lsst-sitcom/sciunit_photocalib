{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Throughputs_LSSTobs_CalSpec\n",
    "\n",
    "Based on Lynne Jones code here: https://rubin-obs.slack.com/archives/C0824CTA335/p1732311332938929\n",
    "\n",
    "Authors:  C. L. Adair, D. L. Tucker, with help from L. Jones, J. Carlin, E. Rykoff, R. Lupton, and others\n",
    "\n",
    "Created:  2024.11.27\n",
    "\n",
    "Updated:  2025.10.20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initial Setup..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import useful python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic python packages\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import warnings\n",
    "from IPython.display import display\n",
    "\n",
    "# LSST Science Pipelines (Stack) packages\n",
    "import lsst.daf.butler as dafButler\n",
    "import lsst.afw.display as afwDisplay\n",
    "\n",
    "# rubin_sim-related packages\n",
    "import rubin_sim.phot_utils as pt\n",
    "from rubin_sim.phot_utils import Bandpass\n",
    "import syseng_throughputs as st\n",
    "from rubin_sim.data import get_data_dir\n",
    "\n",
    "# Astropy-related packages\n",
    "from astropy import units as u\n",
    "from astropy.io import fits\n",
    "from astropy.time import Time\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.table import Table\n",
    "from astropy.stats import sigma_clipped_stats, mad_std\n",
    "\n",
    "\n",
    "# Set a standard figure size to use\n",
    "plt.rcParams['figure.figsize'] = (8.0, 8.0)\n",
    "afwDisplay.setDefaultBackend('matplotlib')\n",
    "\n",
    "# Use Rubin standardized colors/symbols/linestyles for u,g,r,i,z,y\n",
    "from lsst.utils.plotting import (get_multiband_plot_colors,\n",
    "                                 get_multiband_plot_symbols,\n",
    "                                 get_multiband_plot_linestyles)\n",
    "\n",
    "# Set filter warnings to \"ignore\" to avoid a lot of \"logorrhea\" to the screen:\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T22:08:01.355122Z",
     "iopub.status.busy": "2024-11-27T22:08:01.354844Z",
     "iopub.status.idle": "2024-11-27T22:08:01.357333Z",
     "shell.execute_reply": "2024-11-27T22:08:01.356928Z",
     "shell.execute_reply.started": "2024-11-27T22:08:01.355107Z"
    }
   },
   "source": [
    "### 1.2 Include user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which repo, collection, instrument, and skymap to use.\n",
    "# See https://rubinobs.atlassian.net/wiki/spaces/DM/pages/48834013/Campaigns#1.1.-ComCam\n",
    "# and https://rubinobs.atlassian.net/wiki/spaces/DM/pages/226656354/LSSTComCam+Intermittent+Cumulative+DRP+Runs\n",
    "\n",
    "#instrument = 'LSSTComCam'\n",
    "#repo = '/repo/dp1'\n",
    "#collections = 'LSSTComCam/DP1'\n",
    "#skymap_name = 'lsst_cells_v1'\n",
    "#day_obs_start = 20241101\n",
    "#day_obs_end = 20241231\n",
    "\n",
    "instrument = 'LSSTCam'\n",
    "repo = '/repo/main'\n",
    "collections = 'LSSTCam/runs/DRP/20250604_20250921/w_2025_39/DM-52645'\n",
    "#collections='LSSTCam/runs/DRP/20250421_20250921/w_2025_41/DM-52836' (missing processed ECDFS z-band???)\n",
    "skymap_name = 'lsst_cells_v1'\n",
    "day_obs_start = 20250401\n",
    "day_obs_end = 20251230\n",
    "\n",
    "# Set environment variable to point to location of the rubin_sim_data \n",
    "#  (per Lynne Jones' Slack message on the #sciunit-photo-calib channel from 26 Nov 2024):\n",
    "os.environ[\"RUBIN_SIM_DATA_DIR\"] = \"/sdf/data/rubin/shared/rubin_sim_data\"\n",
    "\n",
    "# calspec filename\n",
    "calspec_filename = \"./mag_CalSpec.csv\"\n",
    "#Star_Name = Star_Name\n",
    "#Star_Name = \"WDFS1930-52\"\n",
    "#Star_Name = \"NGC6681-1\"\n",
    "#Star_Name = \"WDFS1514+00\"\n",
    "#Star_Name = \"WDFS1206-27\"\n",
    "#Star_Name = \"VB8\"\n",
    "#Star_Name = \"WDFS1055-36\"\n",
    "#Star_Name = \"WDFS1837-70\"\n",
    "Star_Name = \"C26202\"\n",
    "sed_key = 'stiswfcnic_007'\n",
    "#Star_Name = \"WDFS2317-29\"\n",
    "#Star_Name = \"WDFS1434-28\"\n",
    "#Star_Name = \"WDFS1535-77\"\n",
    "\n",
    "# location of the CalSpec SED FITS files...\n",
    "calspec_sed_path = \"~/Downloads\"\n",
    "calspec_sed_path = os.path.expanduser(calspec_sed_path)\n",
    "print(calspec_sed_path)\n",
    "\n",
    "# List of filters to examine\n",
    "flist = ['u','g','r','i','z','y']\n",
    "\n",
    "# Plot symbol colors to use for ugrizy\n",
    "plot_filter_colors_white_background = {'u': '#0c71ff', 'g': '#49be61', 'r': '#c61c00', 'i': '#ffc200', 'z': '#f341a2', 'y': '#5d0000'}\n",
    "\n",
    "# Variables controlling output...\n",
    "verbose = 3         # verbose = 0, 1, 2, 3, ...  Higher numbers mean more output.\n",
    "outputCSV = True    # output CSV files\n",
    "\n",
    "# There was a major change in the DRP pipeline starting with w_2025_05.\n",
    "# See:  https://rubin-obs.slack.com/archives/C07TXQUAXUZ/p1738795935921129\n",
    "post_w_2025_04 = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Define useful classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful class to stop \"Run All\" at a cell \n",
    "#  containing the command \"raise StopExecution\"\n",
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPRECATED, since we are no longer useing icSrc for the _instFlux'es\n",
    "#  (but keep this function around for the time being, just in case...)\n",
    "\n",
    "# Cartesian x,y match with error (per Claude-3.5-Sonnet)\n",
    "\n",
    "def cartesianXYMatchWithError(df1, xcol1, ycol1, df2, xcol2, ycol2, sep_limit=1.0, allMatches=True):\n",
    "    \n",
    "    import numpy as np\n",
    "    from scipy.spatial import cKDTree\n",
    "    import pandas as pd\n",
    "\n",
    "    # Create KD-tree for efficient spatial searching\n",
    "    tree = cKDTree(df2[[xcol2, ycol2]])\n",
    "\n",
    "    # Find nearest neighbors within sep_limit\n",
    "    separations, indices = tree.query(df1[[xcol1, ycol1]],\n",
    "                                  distance_upper_bound=sep_limit)\n",
    "\n",
    "    # Create mask for valid matches (separations less than sep_limit)\n",
    "    valid_matches = separations < sep_limit\n",
    "\n",
    "    # Create merged dataframe using only valid matches\n",
    "    merged_df = pd.concat([\n",
    "        df1[valid_matches].reset_index(drop=True),\n",
    "        df2.iloc[indices[valid_matches]].reset_index(drop=True)\n",
    "        ], axis=1)\n",
    "\n",
    "    # If you want to keep track of the match separations\n",
    "    merged_df['separation'] = separations[valid_matches]\n",
    "\n",
    "    # If you want to keep just the best match, sort by separation \n",
    "    # and keep first occurrence of each df2 index\n",
    "    if allMatches != True:\n",
    "        merged_df = merged_df.sort_values('separation').drop_duplicates(\n",
    "            subset=df2.columns, keep='first'\n",
    "        )\n",
    "\n",
    "    return merged_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Instantiate the butler and registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "butler = dafButler.Butler(repo, collections=collections)\n",
    "registry = butler.registry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Read calspec file and convert to a python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read calspec file and convert to a python dictionary\n",
    "\n",
    "# Read CSV into a DataFrame\n",
    "df = pd.read_csv(calspec_filename)\n",
    "\n",
    "# Convert to list of dictionaries\n",
    "data = df.to_dict(orient=\"records\")\n",
    "\n",
    "# Or: dictionary of dictionaries keyed by Star_Name\n",
    "data_by_star = df.set_index(\"Star_Name\").to_dict(orient=\"index\")\n",
    "\n",
    "print(data_by_star[Star_Name])\n",
    "\n",
    "raDeg = data_by_star[Star_Name][\"raDeg\"]\n",
    "decDeg = data_by_star[Star_Name][\"decDeg\"]\n",
    "\n",
    "# Grab the row dictionary for this star\n",
    "row = data_by_star[Star_Name]\n",
    "\n",
    "# Build dictionary of file names\n",
    "sedfile_dict = {}\n",
    "\n",
    "# Loop over the last three columns\n",
    "for col in [\"STIS\", \"Model\"]:\n",
    "    val = row[col]\n",
    "    if pd.notna(val) and val != \"\":\n",
    "        # strip leading underscore if present\n",
    "        key = val.strip(\"_\")\n",
    "        filename = f\"{row['Name']}_{key}.fits\"\n",
    "        sedfile_dict[key] = os.path.join(calspec_sed_path, filename)\n",
    "\n",
    "print(sedfile_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Estimate expected counts for airmasses X=1.0 to 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Build the hardware and system for ugrizy for Cerro Pachon for airmasses X=1.0-2.5 in steps of 0.1 airmass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/lsst-pst/syseng_throughputs/blob/main/notebooks/InterpolateZeropoint.ipynb\n",
    "\n",
    "defaultDirs = st.setDefaultDirs()\n",
    "if instrument == \"LSSTComCam\":\n",
    "    #Change detectors from (default) LSST to ComCam (ITL CCDs)\n",
    "    defaultDirs['detector'] = defaultDirs['detector'].replace('/joint_minimum', '/itl')\n",
    "\n",
    "airmasses = np.arange(1.0, 2.6, 0.1).round(2)\n",
    "\n",
    "system = {}\n",
    "for x in airmasses:\n",
    "    #atmos files temporarily inaccessible from /sdf/data/rubin/shared/rubin_sim_data/throughputs/atmos;\n",
    "    # using a temporary solution here:\n",
    "    #atmosDir = '/home/d/dltucker/DATA/rubin_sim_data_throughputs/throughputs/atmos'\n",
    "    #atmos = st.readAtmosphere(atmosDir, atmosFile=f'atmos_{x*10 :.0f}_aerosol.dat')\n",
    "    atmos = st.readAtmosphere(os.path.join(get_data_dir(), 'throughputs', 'atmos'), atmosFile=f'atmos_{x*10 :.0f}_aerosol.dat')\n",
    "    h, s = st.buildHardwareAndSystem(defaultDirs, addLosses=True,  atmosphereOverride=atmos)\n",
    "    system[x] = s\n",
    "hardware = h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Plot filter passbands (without the atmospheric component) and the atmospheric transmission for airmasses 1.0, 1.2, 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/lsst-pst/syseng_throughputs/blob/main/notebooks/InterpolateZeropoint.ipynb\n",
    "\n",
    "# Plot only if verbosity level is higher than 2...\n",
    "if verbose > 2:\n",
    "    \n",
    "    colors = plot_filter_colors_white_background\n",
    "    for f in flist:\n",
    "        plt.plot(hardware[f].wavelen, hardware[f].sb, color=colors[f], linestyle=':')\n",
    "    for x in [1.0, 1.2, 2.0]:\n",
    "        #atmos files temporarily inaccessible from /sdf/data/rubin/shared/rubin_sim_data/throughputs/atmos;\n",
    "        # using a temporary solution here:\n",
    "        #atmosDir = '/home/d/dltucker/DATA/rubin_sim_data_throughputs/throughputs/atmos'\n",
    "        #atmos = st.readAtmosphere(atmosDir, atmosFile=f'atmos_{x*10 :.0f}_aerosol.dat')\n",
    "        atmos = st.readAtmosphere(os.path.join(get_data_dir(), 'throughputs', 'atmos'), atmosFile=f'atmos_{x*10 :.0f}_aerosol.dat')\n",
    "        plt.plot(atmos.wavelen, atmos.sb, linestyle='-')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xlim(300, 1100)\n",
    "    plt.xlabel(\"Wavelength (nm)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Read in the CalSpec SED file and translate it into `rubin_sim` format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the sedfile\n",
    "\n",
    "print(sed_key, sedfile_dict[sed_key])\n",
    "    \n",
    "# Read the SED file associated with this SED\n",
    "sedfile = sedfile_dict[sed_key]\n",
    "seddata = fits.getdata(sedfile)\n",
    "\n",
    "# Transform the SED data into rubin_sim format\n",
    "wavelen = seddata['WAVELENGTH'] * u.angstrom.to(u.nanometer) # This is in angstroms - need in nanometers\n",
    "flambda = seddata['FLUX'] / (u.angstrom.to(u.nanometer)) # this is in erg/sec/cm^^2/ang but we want /nm     \n",
    "sed = pt.Sed(wavelen=wavelen, flambda=flambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot star spectrum (in normalized photon flux) with filter passbands\n",
    "\n",
    "filter_colors = get_multiband_plot_colors()\n",
    "filter_symbols = get_multiband_plot_symbols()\n",
    "filter_linestyles = get_multiband_plot_linestyles()\n",
    "\n",
    "x = 1.2\n",
    "#atmos = st.readAtmosphere(atmosDir, atmosFile=f'atmos_{x*10 :.0f}_aerosol.dat')\n",
    "atmos = st.readAtmosphere(os.path.join(get_data_dir(), 'throughputs', 'atmos'), atmosFile=f'atmos_{x*10 :.0f}_aerosol.dat')\n",
    "\n",
    "# Create empty list to store legend handles\n",
    "legend_handles = []\n",
    "\n",
    "for f in flist:\n",
    "    # Store the line handle when plotting\n",
    "    line, = plt.plot(hardware[f].wavelen, hardware[f].sb*atmos.sb, \n",
    "                    color=filter_colors[f], \n",
    "                    linestyle=filter_linestyles[f],\n",
    "                    label=f'{f}-band')  # Add label for each filter\n",
    "    legend_handles.append(line)\n",
    "\n",
    "# Constants\n",
    "h = 6.626e-34  # Planck constant in J*s\n",
    "c = 2.998e8    # Speed of light in m/s\n",
    "\n",
    "# Calculate photon flux\n",
    "wavelen_m = wavelen * 1e-9\n",
    "flambda_joules = flambda * 1e-7\n",
    "fphoton = flambda_joules * wavelen_m / (h * c)\n",
    "\n",
    "# Store the stellar flux line handle\n",
    "label = \"\"\"%s spectrum\"\"\" % (Star_Name)\n",
    "stellar_line, = plt.plot(wavelen, fphoton/max(fphoton), \n",
    "                        color='darkgrey', \n",
    "                        linestyle='-',\n",
    "                        label=label)\n",
    "legend_handles.append(stellar_line)\n",
    "\n",
    "plt.xlim([300., 1100.])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel(\"Wavelength (nm)\")\n",
    "plt.ylabel(\"Bandpass Total Throughput;  Stellar Photon Flux (normalized)\")\n",
    "\n",
    "# Add the legend\n",
    "plt.legend(handles=legend_handles, loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Define the photometric parameters to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phot_params = pt.PhotometricParameters(exptime=30, nexp=1, gain=1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Calculate the expected counts for CalSpec star for the given photometric parameters over the airmass range of X=1.0-2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, calculate the expected counts using the standard 30-second exposure time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {}\n",
    "for f in flist:\n",
    "    counts[f] = []\n",
    "    for x in airmasses:\n",
    "        counts[f].append(sed.calc_adu(system[x][f], phot_params))\n",
    "    counts[f] = np.array(counts[f])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts = pd.DataFrame(counts, index=airmasses)\n",
    "\n",
    "if verbose > 0:\n",
    "    display(df_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Output results to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if outputCSV:\n",
    "    sedfile_base = os.path.splitext(os.path.basename(sedfile))[0]\n",
    "    outputFile = f\"{instrument}.{sedfile_base}.expected_counts.csv\"\n",
    "    print(f\"Outputting expected counts to {outputFile}\")\n",
    "    df_counts.to_csv(outputFile, index=True)  #  Here, we want to keep the index for the DataFrame, which, in this case, is the airmass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Create tophat passbands covering the same range of wavelengths of the original filter passbands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "x=1.2\n",
    "system_tophat = {}\n",
    "for f in flist:\n",
    "    system_tophat[f] = deepcopy(system[x][f])\n",
    "    system_tophat[f].sb = np.where(system_tophat[f].sb >= 0.001, 1.0, 0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot both the original (X=1.2) passbands and the tophat passbands together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_colors = get_multiband_plot_colors()\n",
    "filter_symbols = get_multiband_plot_symbols()\n",
    "filter_linestyles = get_multiband_plot_linestyles()\n",
    "\n",
    "x = 1.2\n",
    "\n",
    "# Create empty list to store legend handles\n",
    "legend_handles = []\n",
    "\n",
    "for f in flist:\n",
    "    line, = plt.plot(system[x][f].wavelen, system[x][f].sb, \n",
    "                    color=filter_colors[f], \n",
    "                    linestyle=filter_linestyles[f],\n",
    "                    label=f'{f}-band original')  # Add label for each filter\n",
    "    legend_handles.append(line)\n",
    "\n",
    "    line, = plt.plot(system_tophat[f].wavelen, system_tophat[f].sb, \n",
    "                    color=filter_colors[f], \n",
    "                    linestyle='-',\n",
    "                    label=f'{f}-band tophat')  # Add label for each filter\n",
    "    legend_handles.append(line)\n",
    "\n",
    "\n",
    "plt.xlim([300., 1100.])\n",
    "plt.ylim([0.0, 1.1])\n",
    "plt.xlabel(\"Wavelength (nm)\")\n",
    "plt.ylabel(\"Bandpass Total Throughput\")\n",
    "\n",
    "# Add the legend\n",
    "plt.legend(handles=legend_handles, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.tight_layout()  # adjust layout so labels donâ€™t get cut off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Calculate the expected counts for CalSpec star for the given photometric parameters for the tophat passbands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_tophat = {}\n",
    "for f in flist:\n",
    "    counts_tophat[f] = float(sed.calc_adu(system_tophat[f], phot_params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_tophat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T23:21:19.439055Z",
     "iopub.status.busy": "2024-11-27T23:21:19.438762Z",
     "iopub.status.idle": "2024-11-27T23:21:19.441338Z",
     "shell.execute_reply": "2024-11-27T23:21:19.440924Z",
     "shell.execute_reply.started": "2024-11-27T23:21:19.439040Z"
    }
   },
   "source": [
    "## 3. Query USDF Butler for exposures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Create query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from vv-team-notebooks/reports/TargetReport.ipynb\n",
    "\n",
    "# Build WHERE clause\n",
    "\n",
    "where = f\"instrument='{instrument}' AND day_obs>={day_obs_start} AND day_obs<={day_obs_end}\"\n",
    "\n",
    "# Query all exposure records in one go\n",
    "results = list(registry.queryDimensionRecords(\"exposure\", where=where))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Check that there are results; stop execution if there are none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from vv-team-notebooks/reports/TargetReport.ipynb\n",
    "\n",
    "# Stop executing if there are no results returned:\n",
    "\n",
    "n_results = len(results)\n",
    "if n_results == 0:\n",
    "    raise StopExecution\n",
    "else:\n",
    "    print(f\"There are {n_results} results returned from querying the butler for instrument {instrument} \"\n",
    "          f\"between dates {day_obs_start} and {day_obs_end} (inclusive).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Extract all rows into a list of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from vv-team-notebooks/reports/TargetReport.ipynb\n",
    "\n",
    "rows = []\n",
    "for info in results:\n",
    "    try:\n",
    "        rows.append((\n",
    "            info.id,\n",
    "            info.obs_id,\n",
    "            info.day_obs,\n",
    "            info.seq_num,\n",
    "            info.timespan.begin,\n",
    "            info.timespan.end,\n",
    "            info.observation_type,\n",
    "            info.observation_reason,\n",
    "            info.target_name,\n",
    "            info.physical_filter,\n",
    "            info.zenith_angle,\n",
    "            info.exposure_time,\n",
    "            info.tracking_ra,\n",
    "            info.tracking_dec,\n",
    "            info.sky_angle,\n",
    "            info.azimuth,\n",
    "            info.zenith_angle,\n",
    "            info.science_program\n",
    "        ))\n",
    "    except Exception as e:\n",
    "        # Fallback values if timespan is missing or broken\n",
    "        rows.append((\n",
    "            info.id,\n",
    "            info.obs_id,\n",
    "            info.day_obs,\n",
    "            info.seq_num,\n",
    "            pd.to_datetime(\"2021-01-01 00:00:00.00\"),\n",
    "            pd.to_datetime(\"2051-01-01 00:00:00.00\"),\n",
    "            info.observation_type,\n",
    "            info.observation_reason,\n",
    "            info.target_name,\n",
    "            info.physical_filter,\n",
    "            info.zenith_angle,\n",
    "            info.exposure_time,\n",
    "            info.tracking_ra,\n",
    "            info.tracking_dec,\n",
    "            info.sky_angle,\n",
    "            info.azimuth,\n",
    "            info.zenith_angle,\n",
    "            info.science_program\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Build DataFrame in one shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from vv-team-notebooks/reports/TargetReport.ipynb\n",
    "\n",
    "df_exp = pd.DataFrame(rows, columns=[\n",
    "    'id', 'obs_id', 'day_obs', 'seq_num',\n",
    "    'time_start', 'time_end', 'type', 'reason',\n",
    "    'target', 'filter', 'zenith_angle',\n",
    "    'expos', 'ra', 'dec', 'skyangle',\n",
    "    'azimuth', 'zenith', 'science_program'\n",
    "])\n",
    "\n",
    "# Display current version of df_exp\n",
    "#df_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Clean up DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from vv-team-notebooks/reports/TargetReport.ipynb\n",
    "\n",
    "# Compute JD and MJD vectorized\n",
    "t_start = Time(df_exp['time_start'].tolist(), scale='tai')\n",
    "df_exp['jd'] = t_start.jd\n",
    "df_exp['mjd'] = t_start.mjd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from vv-team-notebooks/reports/TargetReport.ipynb\n",
    "\n",
    "# Re-cast the `id`, `day_obs`, and `seq_num` rows as `int`'s:\n",
    "df_exp = df_exp.astype({\"id\": int,'day_obs': int,'seq_num':int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from vv-team-notebooks/reports/TargetReport.ipynb\n",
    "\n",
    "# Replace `NaN`'s in the `ra` and `dec` columns with zero.  \n",
    "# (`NaN`'s in `ra`, `dec` wreak havoc for the healpix tools defined in Section 1.2 above.) \n",
    "# ***(Maybe no longer necessary?)***\n",
    "\n",
    "df_exp['ra'] = df_exp['ra'].fillna(0)\n",
    "df_exp['dec'] = df_exp['dec'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Add airmass to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an airmass to df_exp...\n",
    "\n",
    "df_exp['airmass'] = np.round(1./np.cos(np.deg2rad(df_exp['zenith_angle'])), decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printout zenith angle and airmass if verbosity level is greater than 1...\n",
    "if verbose > 1:\n",
    "    display(df_exp[['zenith_angle','airmass']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Extract just \"science\" exposures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a `DataFrame` containing just the science exposures:\n",
    "df_sci = df_exp[df_exp.type == 'science']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at columns for the (exposure/visit) id, zenith_angle, and airmass, \n",
    "#  but only if verbosity level is greater than 1:\n",
    "if verbose > 1:\n",
    "    display(df_sci[['id', 'zenith_angle','airmass']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Remove any exposures in the \"bad visit\" list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8.1 Read in \"bad visit\" list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if instrument == \"LSSTComCam\":\n",
    "    df_bad_visits=Table.read(\"https://raw.githubusercontent.com/lsst-dm/excluded_visits/refs/heads/main/LSSTComCam/bad.ecsv\").to_pandas()\n",
    "    #df_bad_visits.rename(columns={'exposure': 'visit'}, inplace=True)\n",
    "else: \n",
    "    df_bad_visits=Table.read(\"https://raw.githubusercontent.com/lsst-dm/excluded_visits/refs/heads/main/LSSTCam/bad.ecsv\").to_pandas()\n",
    "\n",
    "# Look at bad visits table, but only if verbosity level is greater than 0:\n",
    "if verbose > 0:\n",
    "    display(df_bad_visits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8.2 Remove from df_sci any exposures found in df_bad_visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sci = df_sci[~df_sci['id'].isin(df_bad_visits['exposure'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at columns for the (exposure/visit) id, zenith_angle, and airmass, \n",
    "#  but only if verbosity level is greater than 0:\n",
    "if verbose > 0:\n",
    "    display(df_sci[['id', 'zenith_angle','airmass']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 Save results as a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if outputCSV:\n",
    "    outputFile = f\"{instrument}.sci_exps.{day_obs_start}-{day_obs_end}.csv\"\n",
    "    print(f\"Outputting exposures to {outputFile}\")\n",
    "    df_sci.to_csv(outputFile, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.10 Create a Pandas DataFrame from df_sci that just contains the visit id, exposure time, zenith angle, and airmass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sci_airmass = df_sci[['id', 'expos', 'zenith_angle','airmass']].copy(deep=True)\n",
    "df_sci_airmass.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Look at pandas dataframe, but only if verbosity level is greater than 0:\n",
    "if verbose > 0:\n",
    "    display(df_sci_airmass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Query USDF Butler for measurements of CalSpec star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Find the `dataId`'s for all `visit_image`'s in this repo/collection that overlap the sky position of CalSpec star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetRefs = butler.query_datasets(\"visit_image\", where=\"visit_detector_region.region OVERLAPS POINT(ra, dec)\",\n",
    "                                    bind={\"ra\": raDeg, \"dec\": decDeg})\n",
    "\n",
    "for i, ref in enumerate(datasetRefs):    \n",
    "    print(i, ref.dataId)\n",
    "    if ((verbose < 2) & (i >= 10)): \n",
    "        print(\"...\")\n",
    "        break\n",
    "\n",
    "\n",
    "print(f\"\\nFound {len(datasetRefs)} visit_images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Create a pandas Dataframe containing the `source2` info for all these `visit_image`'s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Loop over the `datasetRefs` again, grabbing the contents of the `source2` table for each `ref` and combining into all into one big pandas DataFrame.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference CALSPEC star coordinates\n",
    "ref_coord = SkyCoord(ra=raDeg*u.degree, dec=decDeg*u.degree)\n",
    "\n",
    "src_list = []\n",
    "\n",
    "for i, ref in enumerate(datasetRefs):\n",
    "    dataId = {'visit': ref.dataId['visit'], 'detector': ref.dataId['detector']}\n",
    "    src = butler.get('source2', dataId=dataId).to_pandas()\n",
    "#    src = butler.get('recalibrated_star_detector', dataId=dataId).to_pandas()\n",
    "# NOTE - source2 has more matches and gives a slightly different offset to recalibrated - which is going away soon (less than 2 mmag)\n",
    "\n",
    "    # Apply \"good measurement\" mask immediately\n",
    "    mask = (~src.pixelFlags_bad) & (~src.pixelFlags_saturated) & \\\n",
    "           (~src.extendedness_flag)\n",
    "    src_cleaned = src[mask]\n",
    "\n",
    "    # Compute separations to CALSPEC star\n",
    "    df_coords = SkyCoord(ra=src_cleaned['ra'].values*u.degree,\n",
    "                         dec=src_cleaned['dec'].values*u.degree)\n",
    "    separations = ref_coord.separation(df_coords)\n",
    "\n",
    "    # Keep only sources within 60 arcsec (for median aperture corrections later)\n",
    "    mask_sep = separations < 60.0*u.arcsec\n",
    "    nearby = src_cleaned[mask_sep].copy()\n",
    "    nearby['separation_calspec'] = separations[mask_sep].arcsec\n",
    "    \n",
    "    if not nearby.empty:\n",
    "        #best = nearby.sort_values('separation_calspec').iloc[[0]]\n",
    "        #src_list.append(best)\n",
    "        src_list.append(nearby)\n",
    "        if ((verbose >= 2) | (i < 10)): \n",
    "            #print(f\"{i} Visit {ref.dataId['visit']}, Detector {ref.dataId['detector']}: \"\n",
    "            #      f\"Found {len(best)} candidate matches.\")\n",
    "            print(f\"{i} Visit {ref.dataId['visit']}, Detector {ref.dataId['detector']}: \"\n",
    "                  f\"Found {len(nearby)} candidate matches.\")\n",
    "        if ((verbose < 2) & (i == 10)): \n",
    "            print(\"...\")\n",
    "            \n",
    "# Concatenate only the small filtered tables\n",
    "if src_list:\n",
    "    src_all = pd.concat(src_list, ignore_index=True)\n",
    "    print(f\"\\nTotal combined catalog contains {len(src_all)} candidate sources.\")\n",
    "else:\n",
    "    print(\"No matches found within 60 arcsec.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show resulting pandas dataframe, but only if verbosity level is greater than 1:\n",
    "if verbose > 1:\n",
    "    display(src_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 Add exposure time, zenith distance, and airmass to src_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_all_tmp = pd.merge(src_all, df_sci_airmass, left_on='visit', right_on='id')\n",
    "src_all_tmp.drop('id', axis=1, inplace=True)\n",
    "# Remove any rows for which airmass is a NaN\n",
    "src_all_tmp.dropna(subset=['airmass'], inplace=True)\n",
    "src_all = src_all_tmp\n",
    "\n",
    "# Show resulting pandas dataframe, but only if verbosity level is greater than 0:\n",
    "if verbose > 0:\n",
    "    display(src_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3 Add fractional flux error `apFlux_12_0_instFracFluxErr` to src_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_all['apFlux_12_0_instFracFluxErr'] = src_all['apFlux_12_0_instFluxErr']/src_all['apFlux_12_0_instFlux']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if verbose > 2:\n",
    "    for c in src_all.columns:\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if verbose > 2:\n",
    "    for c in src_all.filter(regex='_inst').columns:\n",
    "        print(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3 Save `src_all` as a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if outputCSV:\n",
    "    outputFile = f\"{instrument}.{Star_Name}.source2.csv\"\n",
    "    print(f\"Outputting observed source2 table results for {Star_Name} to {outputFile}\")\n",
    "    src_all.to_csv(outputFile, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Calculate psf to total flux aperture magnitudes on a per-visit basis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 For historical reasons, define df_match to be src_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A TEMPORARY KLUDGE!\n",
    "df_match = src_all.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Calculate values for per-visit `local_photoCalib` and use these values to back-calculate `psfFlux_inst` from `psfFlux`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***We do this since neither `psfFlux_inst` nor `local_photoCalib` seems to be in `source2`.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1 Calculate values for per-visit `local_photoCalib`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the 12-, 17-, 35-, and 50-pixel aperture fluxes. \n",
    "\n",
    "(Note that the values for `local_photoCalib` calculated from each of these aperture fluxes are (nearly) identical, with any differences typcially being at the 0.00001 level or smaller, most likely due to the clipping process in the calculation.  This is a good sign that `local_photoCalib` is well calculated.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kudos to CoPilot+GPT5\n",
    "\n",
    "# Define the apertures you want to process\n",
    "apertures = {\n",
    "    \"12_0\": (\"ap12Flux\", \"apFlux_12_0_instFlux\", \"apFlux_12_0_flag\"),\n",
    "    \"17_0\": (\"ap17Flux\", \"apFlux_17_0_instFlux\", \"apFlux_17_0_flag\"),\n",
    "    \"35_0\": (\"ap35Flux\", \"apFlux_35_0_instFlux\", \"apFlux_35_0_flag\"),\n",
    "    \"50_0\": (\"ap50Flux\", \"apFlux_50_0_instFlux\", \"apFlux_50_0_flag\"),\n",
    "}\n",
    "\n",
    "# Dictionary to hold median calibration DataFrames\n",
    "median_dfs = []\n",
    "\n",
    "for ap_label, (flux_col, instflux_col, flag_col) in apertures.items():\n",
    "\n",
    "    # Create the local calibration ratio column\n",
    "    calib_col = f\"local_photoCalib_{ap_label}\"\n",
    "    df_match[calib_col] = df_match[flux_col] / df_match[instflux_col]\n",
    "\n",
    "    # Create mask\n",
    "    #  Create a mask to cull sources with \"bad\" measurements.\n",
    "    mask1 = (~df_match.pixelFlags_bad) & \\\n",
    "            (~df_match.pixelFlags_saturated) & \\\n",
    "            (~df_match.extendedness_flag) & \\\n",
    "            (~df_match[flag_col])  \n",
    "    #  Create an another mask to cull sources that are too faint or (possibly) too bright.\n",
    "    flux_min = df_match[mask1][instflux_col].quantile(0.75)\n",
    "    flux_max = df_match[mask1][instflux_col].quantile(0.95)\n",
    "    # Combine masks\n",
    "    mask = mask1 & \\\n",
    "        (df_match[instflux_col] >= flux_min) & \\\n",
    "        (df_match[instflux_col] < flux_max) & \\\n",
    "        (df_match.apFlux_12_0_instFracFluxErr >= 0.00) & \\\n",
    "        (df_match.apFlux_12_0_instFracFluxErr < 0.02)\n",
    "\n",
    "\n",
    "    # Compute median per visit, ignoring NaNs\n",
    "    median_series = (\n",
    "        df_match[mask]\n",
    "        .groupby(\"visit\")[calib_col]\n",
    "        .agg(lambda x: np.nanmedian(x))\n",
    "    )\n",
    "\n",
    "    # Convert to DataFrame and rename column\n",
    "    median_df = median_series.reset_index().rename(\n",
    "        columns={calib_col: calib_col}\n",
    "    )\n",
    "    median_dfs.append(median_df)\n",
    "\n",
    "# Merge all median DataFrames on 'visit'\n",
    "from functools import reduce\n",
    "df_median_local_photoCalib = reduce(\n",
    "    lambda left, right: pd.merge(left, right, on=\"visit\", how=\"outer\"),\n",
    "    median_dfs\n",
    ")\n",
    "\n",
    "# Optional: inspect\n",
    "if verbose > 1:\n",
    "    display(df_median_local_photoCalib)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note:  the above procedure also appends these local_photoCalib_* columns to df_match:\n",
    "if verbose > 2:\n",
    "    display(df_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2 Back-calculate psfFlux_inst from psfFlux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use local_photoCalib_12_0 for the conversion\n",
    "df_match['psfFlux_instFlux'] = df_match['psfFlux']/df_match['local_photoCalib_12_0']\n",
    "\n",
    "# Optional: inspect\n",
    "if verbose > 2:\n",
    "    display(df_match)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Create dataframe containing the visit-by-visit median psf-to-total flux aperture corrections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column containing the aperture-to-total flux aperture correction for each individual source.\n",
    "\n",
    "#  We will use 'psfFlux_instFlux' as our primary instrumental flux measurement.\n",
    "#  We will take 'apFlux_50_0_instFlux' as the total flux.\n",
    "df_match['apCorrTot'] = df_match['apFlux_50_0_instFlux'] / df_match['psfFlux_instFlux']\n",
    "\n",
    "# Create a mask to cull sources with \"bad\" measurements.\n",
    "mask1 = (~df_match.pixelFlags_bad) & \\\n",
    "        (~df_match.pixelFlags_saturated) & \\\n",
    "        (~df_match.extendedness_flag) & \\\n",
    "        (~df_match.apFlux_12_0_flag) & \\\n",
    "        (~df_match.apFlux_50_0_flag)  \n",
    "\n",
    "# Create an another mask to cull sources that are too faint or (possibly) too bright.\n",
    "flux_min = df_match[mask1]['psfFlux_instFlux'].quantile(0.75)\n",
    "flux_max = df_match[mask1]['psfFlux_instFlux'].quantile(0.95)\n",
    "mask = mask1 & \\\n",
    "        (df_match.psfFlux_instFlux >= flux_min) & \\\n",
    "        (df_match.psfFlux_instFlux < flux_max)  & \\\n",
    "        (df_match.apFlux_12_0_instFracFluxErr >= 0.00) & \\\n",
    "        (df_match.apFlux_12_0_instFracFluxErr < 0.01)\n",
    "\n",
    "# Calculate median ratio per visit, ignoring NaNs\n",
    "median_apCorrTots = df_match[mask].groupby('visit')['apCorrTot'].agg(lambda x: np.nanmedian(x))\n",
    "\n",
    "# Create a pandas DataFrame out of this pandas Series\n",
    "df_median_apCorrTots = median_apCorrTots.reset_index()\n",
    "\n",
    "# Rename `apCorrTot` to `apCorrTot_median` in df_median_apCorrTots\n",
    "df_median_apCorrTots.rename(columns={'apCorrTot': 'apCorrTot_median'}, inplace=True)\n",
    "\n",
    "## Remove the original apCorrTot column from df_match\n",
    "#df_match.drop('apCorrTot', axis=1, inplace=True)\n",
    "\n",
    "# Show the dataframe of median apCorrTots by visit id, \n",
    "#  but only if verbosity level is greater than 1:\n",
    "if verbose > 1:\n",
    "    display(df_median_apCorrTots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Add the visit-by-visit median aperture corrections to the `df_match`pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_match = df_match.merge(df_median_apCorrTots, on='visit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display result sorted in ascending order of visit (primarily) and RA (secondarily), \n",
    "#  but only if verbosity level is greater than 0:\n",
    "if verbose > 0:\n",
    "    display(df_match.sort_values(by=['visit', 'ra']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Extract the rows containing CalSpec star from the df_match catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on code retrieved from Claude-3.5-Sonnet\n",
    "\n",
    "# Create a mask to cull sources with \"bad\" measurements.\n",
    "mask1 = (~df_match.pixelFlags_bad) & \\\n",
    "        (~df_match.pixelFlags_saturated) & \\\n",
    "        (~df_match.extendedness_flag) & \\\n",
    "        (~df_match.apFlux_12_0_flag) & \\\n",
    "        (~df_match.psfFlux_flag)  \n",
    "\n",
    "mask = mask1 & \\\n",
    "        (df_match.apFlux_12_0_instFracFluxErr >= 0.00) & \\\n",
    "        (df_match.apFlux_12_0_instFracFluxErr < 0.01)\n",
    "\n",
    "\n",
    "# Apply mask, keeping only the \"good\" measurements of `df_match`\n",
    "df_match_cleaned = df_match[mask]\n",
    "\n",
    "# Create SkyCoord object for the coordinates of CalSpec star\n",
    "ref_coord = SkyCoord(ra=raDeg*u.degree, dec=decDeg*u.degree)\n",
    "\n",
    "# Create SkyCoord object for all points in the dataframe\n",
    "df_coords = SkyCoord(ra=df_match_cleaned['ra'].values*u.degree, \n",
    "                     dec=df_match_cleaned['dec'].values*u.degree)\n",
    "\n",
    "# Calculate separations\n",
    "separations = ref_coord.separation(df_coords)\n",
    "\n",
    "# Create mask for points within 3.0 arcseconds\n",
    "mask_sep = separations < 3.0*u.arcsec\n",
    "\n",
    "# Get filtered dataframe\n",
    "nearby_good_df = df_match_cleaned[mask_sep]\n",
    "\n",
    "# If you want to include the separations in the result\n",
    "orig_columns = nearby_good_df.columns\n",
    "nearby_good_df = df_match_cleaned[mask_sep].copy()\n",
    "nearby_good_df['separation_calspec'] = separations[mask_sep].arcsec\n",
    "\n",
    "# Find (and keep) the closet match within the match radius\n",
    "best_df = nearby_good_df.sort_values('separation_calspec').drop_duplicates(subset=orig_columns, keep='first')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the resulting table, but only if verbosity level is greater than 1:\n",
    "if verbose > 1:\n",
    "    display(best_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display only the most relevant columns of the resulting table, \n",
    "# but only if verbosity level is greater than 0:\n",
    "if verbose > 0:\n",
    "    #display(best_df[['visit', 'band', 'expos', 'airmass', 'apFlux_12_0_instFlux', 'apCorrTot_median']])\n",
    "    display(best_df[['visit', 'band', 'expos', 'airmass', 'psfFlux_instFlux', 'apCorrTot_median']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for colname in best_df.columns:\n",
    "#    print(colname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Calculate the ratio of observed to expected throughputs for instrument based on CalSpec star\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Add a column to `best_df` containing the expected counts for CalSpec star based on the contents of `df_counts` created earlier\n",
    "\n",
    "We will use the `interp1d` interpolation function from the `scipy.interpolate` package to perform linear interpolations between the airmasses listed in `df_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on code retrieved from Claude-3.5-Sonnet\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Create a dictionary to store interpolation functions for each band\n",
    "interpolators = {}\n",
    "for band in flist:\n",
    "    interpolators[band] = interp1d(df_counts.index, \n",
    "                                 df_counts[band], \n",
    "                                 kind='linear',\n",
    "                                 bounds_error=False,    # Return nan for out of bounds\n",
    "                                 fill_value=np.nan)\n",
    "\n",
    "# Create new column with interpolated values\n",
    "best_df['total_counts_expected'] = best_df.apply(\n",
    "    lambda row: interpolators[row['band']](row['airmass']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# You can check the results if verbosity level is greater than 0):\n",
    "if verbose > 0:\n",
    "    display(best_df[['visit', 'band', 'airmass', 'total_counts_expected']])\n",
    "\n",
    "# Optional: Check for any NaN values (would indicate airmass outside interpolation range)\n",
    "nan_matches = best_df[best_df['total_counts_expected'].isna()]\n",
    "if len(nan_matches) > 0:\n",
    "    print(\"\\nRows with no matches (airmass out of range):\")\n",
    "    print(nan_matches[['visit', 'band', 'airmass']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Add columns to `best_df` containing the total counts observed and the ratio of total counts observed to total counts expected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_df['total_counts_observed'] = best_df['apCorrTot_median'] * best_df['psfFlux_instFlux']\n",
    "# We will rescale total_counts_observed to a 30-second exposure for those \n",
    "#  (u-band) visits that have a 38-second exposure time:\n",
    "best_df['total_counts_observed'] = (30./best_df['expos'])*best_df['total_counts_observed']\n",
    "best_df['ratio_obs_exp'] = best_df['total_counts_observed'] / best_df['total_counts_expected']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at them..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas to show all rows (but only if verbosity level is greater than 1)...\n",
    "if verbose > 1:\n",
    "    pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to screen the most relevant columns for all rows, \n",
    "#  but only if verbosity level is greater than 0...\n",
    "if verbose > 0:\n",
    "    display(best_df[['visit', 'band', 'airmass', 'expos', 'psfFlux_instFlux', 'apCorrTot_median', 'total_counts_observed', 'total_counts_expected', 'ratio_obs_exp']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset pandas to its default maximum rows to print to screen\n",
    "# (if it had been reset earlier due to verbosity level greater than 1)...\n",
    "if verbose > 1:\n",
    "    pd.reset_option(\"display.max_rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T00:32:08.865742Z",
     "iopub.status.busy": "2024-12-12T00:32:08.865408Z",
     "iopub.status.idle": "2024-12-12T00:32:08.867982Z",
     "shell.execute_reply": "2024-12-12T00:32:08.867498Z",
     "shell.execute_reply.started": "2024-12-12T00:32:08.865727Z"
    }
   },
   "source": [
    "### 7.3 Plot a histogram of the ratio of total counts observed to total counts for each passband"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on code retrieved from Claude-3.5-Sonnet and Poe.com Assistant\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Define colors and transparency for each band\n",
    "colors = plot_filter_colors_white_background\n",
    "alpha = 1.0   # transparency level\n",
    "linewidth = 3 # linewidth for the step histogram lines \n",
    "\n",
    "# Define bins.  Here, we want to look around ratio=1.00+/-0.20 in steps of 0.01\n",
    "#bins = np.arange(0.80, 1.20, 0.01)\n",
    "#bins = np.arange(0.00, 2.00, 0.01)\n",
    "bins = np.arange(0.00, 2.00, 0.02)\n",
    "\n",
    "# Plot histogram for each band\n",
    "for band in flist:\n",
    "    band_data = best_df[best_df['band'] == band]['ratio_obs_exp']\n",
    "    if len(band_data) > 0:  # only plot if we have data for this band\n",
    "        #plt.hist(band_data, bins=bins, alpha=alpha, histtype='step', linewidth=linewidth, \n",
    "        #        label=f'band {band}', color=colors[band],\n",
    "        #        density=False)  # density=True normalizes the area\n",
    "        #plt.hist(band_data, bins=bins, alpha=alpha, histtype='step', linewidth=linewidth, \n",
    "        #        label=f'band {band}', color=filter_colors[band],\n",
    "        #        linestyle=filter_linestyles[band], \n",
    "        #        density=False)  # density=True normalizes the area\n",
    "        plt.hist(band_data, bins=bins, alpha=alpha, histtype='step', linewidth=linewidth, \n",
    "                label=f'band {band}', color=filter_colors[band],\n",
    "                density=False)  # density=True normalizes the area\n",
    "\n",
    "\n",
    "plt.xlabel('Ratio (Observed Counts/Expected Counts)')\n",
    "plt.ylabel('Number')\n",
    "#plt.xlim([0.75, 1.15])\n",
    "plt.xlim([0.75, 1.50])\n",
    "#plt.xlim([0.00, 2.00])\n",
    "\n",
    "plot_title = \"\"\"Distribution of Observed/Expected Total Counts Ratio by Band for %s\"\"\" % (Star_Name)\n",
    "plt.title(plot_title)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Optional: adjust layout to prevent label clipping\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Print summary statistics for each band"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's output a nice table..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on code retrieved from Claude-3.5-Sonnet and from CoPilot+GPT5\n",
    "\n",
    "stats_data = []\n",
    "\n",
    "for band in flist:\n",
    "    # Extract and clean the data\n",
    "    band_data = best_df.loc[best_df['band'] == band, 'ratio_obs_exp']\n",
    "    band_data = pd.to_numeric(band_data, errors='coerce')  # force numeric\n",
    "    band_data = band_data.replace([np.inf, -np.inf], np.nan).dropna().values.astype(float)\n",
    "\n",
    "    if len(band_data) > 0:\n",
    "        # --- Option A: sigma-clipped stats ---\n",
    "        mean_clip, median_clip, std_clip = sigma_clipped_stats(band_data, sigma=3.0, maxiters=5)\n",
    "        stderr_clip = std_clip / np.sqrt(len(band_data))\n",
    "\n",
    "        # --- Option B: robust median/MAD ---\n",
    "        median_robust = np.median(band_data)\n",
    "        std_robust = mad_std(band_data)\n",
    "        stderr_robust = std_robust / np.sqrt(len(band_data))\n",
    "\n",
    "        stats_data.append({\n",
    "            'band': band,\n",
    "            'n_band': len(band_data),\n",
    "            'Mean (clip)': f\"{mean_clip:.3f}\",\n",
    "            'Median (clip)': f\"{median_clip:.3f}\",\n",
    "            'Std (clip)': f\"{std_clip:.3f}\",\n",
    "            'StdErr (clip)': f\"{stderr_clip:.3f}\",\n",
    "            'Median (robust)': f\"{median_robust:.3f}\",\n",
    "            'Std (robust)': f\"{std_robust:.3f}\",\n",
    "            'StdErr (robust)': f\"{stderr_robust:.3f}\"\n",
    "        })\n",
    "    else:\n",
    "        stats_data.append({\n",
    "            'band': band,\n",
    "            'n_band': 0,\n",
    "            'Mean (clip)': 'N/A',\n",
    "            'Median (clip)': 'N/A',\n",
    "            'Std (clip)': 'N/A',\n",
    "            'StdErr (clip)': 'N/A',\n",
    "            'Median (robust)': 'N/A',\n",
    "            'Std (robust)': 'N/A',\n",
    "            'StdErr (robust)': 'N/A'\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "stats_df = pd.DataFrame(stats_data)\n",
    "\n",
    "# Display the full table\n",
    "display(stats_df)\n",
    "#print(stats_df.to_string(index=False))\n",
    "\n",
    "# Display a truncated table\n",
    "display(stats_df[['band', 'n_band', 'Mean (clip)', 'Median (clip)', 'StdErr (clip)']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Calculate the absolute throughputs for instrument based on CalSpec star\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do this by comparing the observed throughputs from the real filters to the expected throughputs from the tophat filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Add columns to `best_df` containing the total expected counts from the tophat filters and the ratio of total counts observed from the real filters to total counts expected from the tophat filters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts_tophat = pd.Series(counts_tophat, name=\"total_counts_expected_tophat\")\n",
    "best_df = best_df.join(df_counts_tophat, on=\"band\")\n",
    "best_df['abs_throughput'] = best_df['total_counts_observed'] / best_df['total_counts_expected_tophat']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas to show all rows (but only if verbosity level is greater than 1)...\n",
    "if verbose > 1:\n",
    "    pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to screen the most relevant columns for all rows, \n",
    "#  but only if verbosity level is greater than 0...\n",
    "if verbose > 0:\n",
    "    display(best_df[['visit', 'band', 'airmass', 'expos', 'total_counts_observed', 'total_counts_expected_tophat', 'abs_throughput']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset pandas to its default maximum rows to print to screen\n",
    "# (if it had been reset earlier due to verbosity level greater than 1)...\n",
    "if verbose > 1:\n",
    "    pd.reset_option(\"display.max_rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Plot a histogram and a scatter plot of the absolute throughputs for each passband"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on code retrieved from Claude-3.5-Sonnet and Poe.com Assistant\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Define colors and transparency for each band\n",
    "colors = plot_filter_colors_white_background\n",
    "alpha = 1.0   # transparency level\n",
    "linewidth = 3 # linewidth for the step histogram lines \n",
    "\n",
    "# Define bins.  Here, we want to look around ratio=1.00+/-0.20 in steps of 0.01\n",
    "#bins = np.arange(0.80, 1.20, 0.01)\n",
    "#bins = np.arange(0.00, 2.00, 0.01)\n",
    "bins = np.arange(0.00, 2.00, 0.02)\n",
    "\n",
    "# Plot histogram for each band\n",
    "for band in flist:\n",
    "    band_data = best_df[best_df['band'] == band]['abs_throughput']\n",
    "    if len(band_data) > 0:  # only plot if we have data for this band\n",
    "        #plt.hist(band_data, bins=bins, alpha=alpha, histtype='step', linewidth=linewidth, \n",
    "        #        label=f'band {band}', color=colors[band],\n",
    "        #        density=False)  # density=True normalizes the area\n",
    "        #plt.hist(band_data, bins=bins, alpha=alpha, histtype='step', linewidth=linewidth, \n",
    "        #        label=f'band {band}', color=filter_colors[band],\n",
    "        #        linestyle=filter_linestyles[band], \n",
    "        #        density=False)  # density=True normalizes the area\n",
    "        plt.hist(band_data, bins=bins, alpha=alpha, histtype='step', linewidth=linewidth, \n",
    "                label=f'band {band}', color=filter_colors[band],\n",
    "                density=False)  # density=True normalizes the area\n",
    "\n",
    "\n",
    "plt.xlabel('Absolute Throughput')\n",
    "plt.ylabel('Number')\n",
    "plt.xlim([0.00, 1.00])\n",
    "\n",
    "plot_title = \"\"\"Distribution of Estimated Absolute Throughputs by Band using %s\"\"\" % (Star_Name)\n",
    "plt.title(plot_title)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Optional: adjust layout to prevent label clipping\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_colors = get_multiband_plot_colors()\n",
    "filter_symbols = get_multiband_plot_symbols()\n",
    "filter_linestyles = get_multiband_plot_linestyles()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Loop over filters\n",
    "for band in flist:\n",
    "    band_data = best_df[best_df['band'] == band]\n",
    "    if len(band_data) > 0:\n",
    "        plt.scatter(\n",
    "            band_data[\"airmass\"],\n",
    "            band_data[\"abs_throughput\"],\n",
    "            label=f\"band {band}\",\n",
    "            marker=filter_symbols[band], \n",
    "            c=filter_colors[band],\n",
    "            alpha=0.8\n",
    "        )\n",
    "\n",
    "plt.xlabel(\"Airmass\")\n",
    "plt.ylabel(\"Passband Absolute Throughput\")\n",
    "plt.title(f\"Absolute Throughput vs. Airmass by Band using {Star_Name}\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Print summary statistics for each band"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's output a nice table..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on code retrieved from Claude-3.5-Sonnet and from CoPilot+GPT5\n",
    "\n",
    "stats_data = []\n",
    "\n",
    "for band in flist:\n",
    "    # Extract and clean the data\n",
    "    band_data = best_df.loc[best_df['band'] == band, 'abs_throughput']\n",
    "    band_data = pd.to_numeric(band_data, errors='coerce')  # force numeric\n",
    "    band_data = band_data.replace([np.inf, -np.inf], np.nan).dropna().values.astype(float)\n",
    "\n",
    "    if len(band_data) > 0:\n",
    "        # --- Option A: sigma-clipped stats ---\n",
    "        mean_clip, median_clip, std_clip = sigma_clipped_stats(band_data, sigma=3.0, maxiters=5)\n",
    "        stderr_clip = std_clip / np.sqrt(len(band_data))\n",
    "\n",
    "        # --- Option B: robust median/MAD ---\n",
    "        median_robust = np.median(band_data)\n",
    "        std_robust = mad_std(band_data)\n",
    "        stderr_robust = std_robust / np.sqrt(len(band_data))\n",
    "\n",
    "        stats_data.append({\n",
    "            'band': band,\n",
    "            'n_band': len(band_data),\n",
    "            'Mean (clip)': f\"{mean_clip:.3f}\",\n",
    "            'Median (clip)': f\"{median_clip:.3f}\",\n",
    "            'Std (clip)': f\"{std_clip:.3f}\",\n",
    "            'StdErr (clip)': f\"{stderr_clip:.3f}\",\n",
    "            'Median (robust)': f\"{median_robust:.3f}\",\n",
    "            'Std (robust)': f\"{std_robust:.3f}\",\n",
    "            'StdErr (robust)': f\"{stderr_robust:.3f}\"\n",
    "        })\n",
    "    else:\n",
    "        stats_data.append({\n",
    "            'band': band,\n",
    "            'n_band': 0,\n",
    "            'Mean (clip)': 'N/A',\n",
    "            'Median (clip)': 'N/A',\n",
    "            'Std (clip)': 'N/A',\n",
    "            'StdErr (clip)': 'N/A',\n",
    "            'Median (robust)': 'N/A',\n",
    "            'Std (robust)': 'N/A',\n",
    "            'StdErr (robust)': 'N/A'\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "stats_df = pd.DataFrame(stats_data)\n",
    "\n",
    "# Display the full table\n",
    "display(stats_df)\n",
    "#print(stats_df.to_string(index=False))\n",
    "\n",
    "# Display a truncated table\n",
    "display(stats_df[['band', 'n_band', 'Mean (clip)', 'Median (clip)', 'StdErr (clip)']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's stop here for now:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Stopping here...\")\n",
    "raise StopExecution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
