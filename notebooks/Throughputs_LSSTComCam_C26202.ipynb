{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Throughputs_LSSTComCam_c26202\n",
    "\n",
    "Based on Lynne Jones code here: https://rubin-obs.slack.com/archives/C0824CTA335/p1732311332938929\n",
    "\n",
    "Created:  2024.11.27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initial Setup..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import useful python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic python packages\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "# LSST Science Pipelines (Stack) packages\n",
    "import lsst.daf.butler as dafButler\n",
    "import lsst.afw.display as afwDisplay\n",
    "\n",
    "# rubin_sim-related packages\n",
    "import rubin_sim.phot_utils as pt\n",
    "import syseng_throughputs as st\n",
    "from rubin_sim.data import get_data_dir\n",
    "\n",
    "# Astropy-related packages\n",
    "from astropy import units as u\n",
    "from astropy.io import fits\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.table import Table\n",
    "\n",
    "# Set a standard figure size to use\n",
    "plt.rcParams['figure.figsize'] = (8.0, 8.0)\n",
    "afwDisplay.setDefaultBackend('matplotlib')\n",
    "\n",
    "# Set filter warnings to \"ignore\" to avoid a lot of \"logorrhea\" to the screen:\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T22:08:01.355122Z",
     "iopub.status.busy": "2024-11-27T22:08:01.354844Z",
     "iopub.status.idle": "2024-11-27T22:08:01.357333Z",
     "shell.execute_reply": "2024-11-27T22:08:01.356928Z",
     "shell.execute_reply.started": "2024-11-27T22:08:01.355107Z"
    }
   },
   "source": [
    "### 1.2 Include user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which repo, collection, instrument, and skymap to use.\n",
    "# See https://rubinobs.atlassian.net/wiki/spaces/DM/pages/48834013/Campaigns#1.1.-ComCam\n",
    "# and https://rubinobs.atlassian.net/wiki/spaces/DM/pages/226656354/LSSTComCam+Intermittent+Cumulative+DRP+Runs\n",
    "#repo = 'embargo'\n",
    "repo = '/repo/main'\n",
    "\n",
    "#collections = 'LSSTComCam/runs/DRP/20241101_20241113/w_2024_46/DM-47566'\n",
    "#collections = 'LSSTComCam/runs/DRP/20241101_20241120/w_2024_47/DM-47746'\n",
    "#collections = 'LSSTComCam/runs/DRP/20241101_20241204/w_2024_49/DM-47988'\n",
    "#collections = 'LSSTComCam/runs/DRP/DP1/w_2025_03/DM-48478'\n",
    "#collections = 'LSSTComCam/runs/DRP/DP1/w_2025_04/DM-48556'\n",
    "collections = 'LSSTComCam/runs/DRP/DP1/w_2025_05/DM-48666'\n",
    "\n",
    "instrument = 'LSSTComCam'\n",
    "skymap_name = 'lsst_cells_v1'\n",
    "day_obs_start = 20241101\n",
    "day_obs_end = 20241212\n",
    "\n",
    "# Set environment variable to point to location of the rubin_sim_data \n",
    "#  (per Lynne Jones' Slack message on the #sciunit-photo-calib channel from 26 Nov 2024):\n",
    "os.environ[\"RUBIN_SIM_DATA_DIR\"] = \"/sdf/data/rubin/shared/rubin_sim_data\"\n",
    "\n",
    "# Which CalSpec C26202 spectrum FITS file to to use?\n",
    "sedfile = '~/Downloads/c26202_stiswfcnic_007.fits'\n",
    "#sedfile = '~/Downloads/c26202_mod_007.fits'\n",
    "#sedfile = '~/Downloads/c26202_stiswfcnic_006.fits'\n",
    "#sedfile = '~/Downloads/c26202_mod_008.fits'\n",
    "#sedfile = '~/Downloads/c26202_stiswfcnic_007.fits'\n",
    "\n",
    "# RA, DEC of C26202\n",
    "raDeg = 53.136845833333325\n",
    "decDeg = -27.86349444444444\n",
    "\n",
    "# Plot symbol colors to use for ugrizy\n",
    "plot_filter_colors_white_background = {'u': '#0c71ff', 'g': '#49be61', 'r': '#c61c00', 'i': '#ffc200', 'z': '#f341a2', 'y': '#5d0000'}\n",
    "\n",
    "# Variables controlling output...\n",
    "verbose = 3         # verbose = 0, 1, 2, 3, ...  Higher numbers mean more output.\n",
    "outputCSV = False    # output CSV files\n",
    "# There was a major change in the DRP pipeline starting with w_2025_05.\n",
    "# See:  https://rubin-obs.slack.com/archives/C07TXQUAXUZ/p1738795935921129\n",
    "post_w_2025_04 = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Define useful classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cartesian x,y match with error (per Claude-3.5-Sonnet)\n",
    "\n",
    "def cartesianXYMatchWithError(df1, xcol1, ycol1, df2, xcol2, ycol2, sep_limit=1.0, allMatches=True):\n",
    "    \n",
    "    import numpy as np\n",
    "    from scipy.spatial import cKDTree\n",
    "    import pandas as pd\n",
    "\n",
    "    # Create KD-tree for efficient spatial searching\n",
    "    tree = cKDTree(df2[[xcol2, ycol2]])\n",
    "\n",
    "    # Find nearest neighbors within sep_limit\n",
    "    separations, indices = tree.query(df1[[xcol1, ycol1]],\n",
    "                                  distance_upper_bound=sep_limit)\n",
    "\n",
    "    # Create mask for valid matches (separations less than sep_limit)\n",
    "    valid_matches = separations < sep_limit\n",
    "\n",
    "    # Create merged dataframe using only valid matches\n",
    "    merged_df = pd.concat([\n",
    "        df1[valid_matches].reset_index(drop=True),\n",
    "        df2.iloc[indices[valid_matches]].reset_index(drop=True)\n",
    "        ], axis=1)\n",
    "\n",
    "    # If you want to keep track of the match separations\n",
    "    merged_df['separation'] = separations[valid_matches]\n",
    "\n",
    "    # If you want to keep just the best match, sort by separation \n",
    "    # and keep first occurrence of each df2 index\n",
    "    if allMatches != True:\n",
    "        merged_df = merged_df.sort_values('separation').drop_duplicates(\n",
    "            subset=df2.columns, keep='first'\n",
    "        )\n",
    "\n",
    "    return merged_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful class to stop \"Run All\" at a cell \n",
    "#  containing the command \"raise StopExecution\"\n",
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Instantiate the butler and registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "butler = dafButler.Butler(repo, collections=collections)\n",
    "registry = butler.registry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Estimate expected counts for airmasses X=1.0 to 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Build the hardware and system for ugrizy for Cerro Pachon for airmasses X=1.0-2.5 in steps of 0.1 airmass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/lsst-pst/syseng_throughputs/blob/main/notebooks/InterpolateZeropoint.ipynb\n",
    "\n",
    "defaultDirs = st.setDefaultDirs()\n",
    "defaultDirs['detector'] = defaultDirs['detector'].replace('/joint_minimum', '/itl')\n",
    "print(defaultDirs)\n",
    "\n",
    "airmasses = np.arange(1.0, 2.6, 0.1).round(2)\n",
    "\n",
    "system = {}\n",
    "for x in airmasses:\n",
    "    atmos = st.readAtmosphere(os.path.join(get_data_dir(), 'throughputs', 'atmos'), atmosFile=f'atmos_{x*10 :.0f}_aerosol.dat')\n",
    "    h, s = st.buildHardwareAndSystem(defaultDirs, addLosses=True,  atmosphereOverride=atmos)\n",
    "    system[x] = s\n",
    "hardware = h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Plot filter passbands (without the atmospheric component) and the atmospheric transmission for airmasses 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/lsst-pst/syseng_throughputs/blob/main/notebooks/InterpolateZeropoint.ipynb\n",
    "\n",
    "# Plot only if verbosity level is higher than 2...\n",
    "if verbose > 2:\n",
    "    \n",
    "    colors = plot_filter_colors_white_background\n",
    "    for f in 'ugrizy':\n",
    "        plt.plot(hardware[f].wavelen, hardware[f].sb, color=colors[f], linestyle=':')\n",
    "    for x in [1.0, 1.2, 2.0]:\n",
    "        atmos = st.readAtmosphere(os.path.join(get_data_dir(), 'throughputs', 'atmos'), atmosFile=f'atmos_{x*10 :.0f}_aerosol.dat')\n",
    "        plt.plot(atmos.wavelen, atmos.sb, linestyle='-')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xlim(300, 1100)\n",
    "    plt.xlabel(\"Wavelength (nm)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Read in the CalSpec SED file and translate it into `rubin_sim` format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the sedfile file \n",
    "seddata = fits.getdata(sedfile)\n",
    "\n",
    "# Translate to rubin_sim format\n",
    "wavelen = seddata['WAVELENGTH'] * u.angstrom.to(u.nanometer) # This is in angstroms - need in nanometers\n",
    "flambda = seddata['FLUX'] / (u.angstrom.to(u.nanometer)) # this is in erg/sec/cm^^2/ang but we want /nm \n",
    "\n",
    "# Convert to rubin_sim format\n",
    "sed = pt.Sed(wavelen=wavelen, flambda=flambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Define the photometric parameters to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phot_params = pt.PhotometricParameters(exptime=30, nexp=1, gain=1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Calculate the expected counts for C26202 for the given photometric parameters over the airmass range of X=1.0-2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {}\n",
    "for f in 'ugrizy':\n",
    "    counts[f] = []\n",
    "    for x in airmasses:\n",
    "        counts[f].append(sed.calc_adu(system[x][f], phot_params))\n",
    "    counts[f] = np.array(counts[f])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts = pd.DataFrame(counts, index=airmasses)\n",
    "\n",
    "if verbose > 0:\n",
    "    display(df_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Output results to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if outputCSV:\n",
    "    outputFile = 'LSSTComCam_C26202_expected_counts.csv'\n",
    "    df_counts.to_csv(outputFile)  #  Here, we want to keep the index for the DataFrame, which, in this case, is the airmass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T23:21:19.439055Z",
     "iopub.status.busy": "2024-11-27T23:21:19.438762Z",
     "iopub.status.idle": "2024-11-27T23:21:19.441338Z",
     "shell.execute_reply": "2024-11-27T23:21:19.440924Z",
     "shell.execute_reply.started": "2024-11-27T23:21:19.439040Z"
    }
   },
   "source": [
    "## 3. Query USDF Butler for ComCam exposures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Create query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from vv-team-notebooks/reports/TargetReport.ipynb\n",
    "\n",
    "# Query the metadata for the `exposure` dimension, limiting the results to this particular instrument and range of days of observation:\n",
    "query=\"instrument='%s' AND day_obs>=%d AND day_obs<=%d\" % (instrument, day_obs_start, day_obs_end)\n",
    "results = registry.queryDimensionRecords('exposure',where=query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Check that there are results; stop execution if there are none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from vv-team-notebooks/reports/TargetReport.ipynb\n",
    "\n",
    "# Stop executing if there are no results returned:\n",
    "\n",
    "n_results = results.count()\n",
    "\n",
    "if n_results <= 0:\n",
    "    raise StopExecution\n",
    "else:\n",
    "    print(\"\"\"There are %d results returned from querying the butler for instrument %s between dates %d and %d (inclusive).\"\"\" % \n",
    "          (n_results, instrument, day_obs_start, day_obs_end))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Instantiate a DataFrame to contain the exposure information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from vv-team-notebooks/reports/TargetReport.ipynb\n",
    "\n",
    "# Instantiate a pandas `DataFrame` with useful columns available in the `exposure` dimension:\n",
    "df_exp = pd.DataFrame(columns=['id', 'obs_id','day_obs', 'seq_num',\n",
    "                                    'time_start','time_end' ,'type', 'reason', \n",
    "                                    'target','filter','zenith_angle',\n",
    "                                    'expos','ra','dec','skyangle',\n",
    "                                    'azimuth','zenith','science_program',\n",
    "                                    'jd','mjd'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Read the query results into the new Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from vv-team-notebooks/reports/TargetReport.ipynb\n",
    "\n",
    "# Read the query results into the new pandas `DataFrame`:\n",
    "\n",
    "for count, info in enumerate(results):\n",
    "    \n",
    "    try:\n",
    "\n",
    "        df_exp.loc[count] = [info.id, info.obs_id, info.day_obs, info.seq_num, \n",
    "                                  info.timespan.begin.utc.iso,\n",
    "                                  info.timespan.end.utc.iso, \n",
    "                                  info.observation_type, info.observation_reason, info.target_name, \n",
    "                                  info.physical_filter, info.zenith_angle, \n",
    "                                  info.exposure_time,info.tracking_ra, info.tracking_dec, \n",
    "                                  info.sky_angle,info.azimuth ,info.zenith_angle, \n",
    "                                  info.science_program, info.timespan.begin.jd, info.timespan.begin.mjd]\n",
    "\n",
    "    except:\n",
    "    \n",
    "        print(\">>>   Unexpected error:\", sys.exc_info()[0])\n",
    "        info_timespan_begin_to_string = \"2021-01-01 00:00:00.00\"\n",
    "        info_timespan_end_to_string = \"2051-01-01 00:00:00.00\"\n",
    "        info_timespan_begin_jd = 0\n",
    "        info_timespan_begin_mjd = 0\n",
    "        df_exp.loc[count] = [info.id, info.obs_id, info.day_obs, info.seq_num, \n",
    "                                  pd.to_datetime(info_timespan_begin_to_string),\n",
    "                                  pd.to_datetime(info_timespan_end_to_string), \n",
    "                                  info.observation_type, info.observation_reason, info.target_name, \n",
    "                                  info.physical_filter, info.zenith_angle, \n",
    "                                  info.exposure_time,info.tracking_ra, info.tracking_dec, \n",
    "                                  info.sky_angle,info.azimuth ,info.zenith_angle, \n",
    "                                  info.science_program, info_timespan_begin_jd, info_timespan_begin_mjd ]\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Clean up DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from vv-team-notebooks/reports/TargetReport.ipynb\n",
    "\n",
    "# Re-cast the `id`, `day_obs`, and `seq_num` rows as `int`'s:\n",
    "df_exp = df_exp.astype({\"id\": int,'day_obs': int,'seq_num':int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from vv-team-notebooks/reports/TargetReport.ipynb\n",
    "\n",
    "# Replace `NaN`'s in the `ra` and `dec` columns with zero.  \n",
    "# (`NaN`'s in `ra`, `dec` wreak havoc for the healpix tools defined in Section 1.2 above.) \n",
    "# ***(Maybe no longer necessary?)***\n",
    "\n",
    "df_exp['ra'] = df_exp['ra'].fillna(0)\n",
    "df_exp['dec'] = df_exp['dec'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Add airmass to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an airmass to df_exp...\n",
    "\n",
    "df_exp['airmass'] = np.round(1./np.cos(np.deg2rad(df_exp['zenith_angle'])), decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printout zenith angle and airmass if verbosity level is greater than 1...\n",
    "if verbose > 1:\n",
    "    display(df_exp[['zenith_angle','airmass']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Extract just \"science\" exposures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a `DataFrame` containing just the science exposures:\n",
    "df_sci = df_exp[df_exp.type == 'science']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at columns for the (exposure/visit) id, zenith_angle, and airmass, \n",
    "#  but only if verbosity level is greater than 1:\n",
    "if verbose > 1:\n",
    "    display(df_sci[['id', 'zenith_angle','airmass']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Remove any exposures in the \"bad visit\" list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8.1 Read in \"bad visit\" list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bad_visits=Table.read(\"https://raw.githubusercontent.com/lsst-dm/excluded_visits/refs/heads/main/LSSTComCam/bad.ecsv\").to_pandas()\n",
    "#df_bad_visits.rename(columns={'exposure': 'visit'}, inplace=True)\n",
    "\n",
    "# Look at bad visits table, but only if verbosity level is greater than 0:\n",
    "if verbose > 0:\n",
    "    display(df_bad_visits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8.2 Remove from df_sci and exposuress found in df_bad_visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sci = df_sci[~df_sci['id'].isin(df_bad_visits['exposure'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at columns for the (exposure/visit) id, zenith_angle, and airmass, \n",
    "#  but only if verbosity level is greater than 0:\n",
    "if verbose > 0:\n",
    "    display(df_sci[['id', 'zenith_angle','airmass']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 Save results as a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if outputCSV:\n",
    "    outputFile = \"\"\"LSSTComCam_visits_%d-%d.csv\"\"\" % (day_obs_start, day_obs_end)\n",
    "    df_sci.to_csv(outputFile, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.10 Create a Pandas DataFrame from df_sci that just contains the visit id, zenith angle, and airmass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sci_airmass = df_sci[['id', 'zenith_angle','airmass']].copy(deep=True)\n",
    "df_sci_airmass.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Look at pandas dataframe, but only if verbosity level is greater than 0:\n",
    "if verbose > 0:\n",
    "    display(df_sci_airmass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Query USDF Butler for ComCam measurements of C26202"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Find the `dataId`'s for all `calexp`'s in this repo/collection that overlap the sky position of C26202:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetRefs = butler.query_datasets(\"calexp\", where=\"visit_detector_region.region OVERLAPS POINT(ra, dec)\",\n",
    "                                    bind={\"ra\": raDeg, \"dec\": decDeg})\n",
    "\n",
    "# Look datasetRefs, but only if verbosity level is greater than 1:\n",
    "if verbose > 1:\n",
    "    for i, ref in enumerate(datasetRefs):    \n",
    "        print(i, ref.dataId)\n",
    "\n",
    "print(f\"\\nFound {len(datasetRefs)} calexps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Create a pandas Dataframe containing the `sourceTable` info for all these `calexp`'s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Loop over the `datasetRefs` again, grabbing the contents of the `sourceTable` table for each `ref` and combining them into all into one big pandas DataFrame.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_list = []\n",
    "\n",
    "for i, ref in enumerate(datasetRefs):\n",
    "    \n",
    "    # Retrieve sourceTable for this visit & detector...\n",
    "    dataId = {'visit': ref.dataId['visit'], 'detector': ref.dataId['detector']}\n",
    "    src = butler.get('sourceTable', dataId=dataId)\n",
    "    src_list.append(src)\n",
    "    \n",
    "    # Look at visit/detector info, but only if verbosity level is greater than 1:\n",
    "    if verbose > 1:\n",
    "        print(f\"{i} Visit {ref.dataId['visit']}, Detector {ref.dataId['detector']}:  Retrieved catalog of {len(src)} sources.\")\n",
    "\n",
    "src_all = pd.concat(src_list, ignore_index=True)\n",
    "\n",
    "print(\"\")\n",
    "print(f\"Total combined catalog contains {len(src_all)} sources.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show resulting pandas dataframe, but only if verbosity level is greater than 1:\n",
    "if verbose > 1:\n",
    "    display(src_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 Add zenith distance and airmass to src_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_all_tmp = pd.merge(src_all, df_sci_airmass, left_on='visit', right_on='id')\n",
    "src_all_tmp.drop('id', axis=1, inplace=True)\n",
    "# Remove any rows for which airmass is a NaN\n",
    "src_all_tmp.dropna(subset=['airmass'], inplace=True)\n",
    "src_all = src_all_tmp\n",
    "\n",
    "# Show resulting pandas dataframe, but only if verbosity level is greater than 0:\n",
    "if verbose > 0:\n",
    "    display(src_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3 Save `src_all` as a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if outputCSV:\n",
    "    src_all.to_csv('LSSTComCam_C26202_fields.sourceTable.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Create a pandas Dataframe containing the `icSrc` table info for all these `calexp`'s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1 Loop over the `datasetRefs` again, grabbing the contents of the `icSrc` table for each `ref` and combining them into all into one big pandas DataFrame.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icSrc_list = []\n",
    "\n",
    "for i, ref in enumerate(datasetRefs):\n",
    "    \n",
    "    # Retrieve sourceTable for this visit & detector...\n",
    "    try:\n",
    "        dataId = {'visit': ref.dataId['visit'], 'detector': ref.dataId['detector']}\n",
    "\n",
    "        # There was a major change in the DRP pipeline starting with w_2025_05.\n",
    "        # See:  https://rubin-obs.slack.com/archives/C07TXQUAXUZ/p1738795935921129\n",
    "        if post_w_2025_04:\n",
    "            icSrc = butler.get('initial_psf_stars_detector', dataId=dataId)\n",
    "            icSrc_table = icSrc\n",
    "        else:\n",
    "            icSrc = butler.get('icSrc', dataId=dataId)\n",
    "            icSrc_table = icSrc.asAstropy()\n",
    "            \n",
    "        df_icSrc = icSrc_table.to_pandas()\n",
    "        icSrc_list.append(df_icSrc)\n",
    "\n",
    "        # Look at visit/detector info, but only if verbosity level is greater than 1:\n",
    "        if verbose > 1:\n",
    "            print(f\"{i} Visit {ref.dataId['visit']}, Detector {ref.dataId['detector']}:  Retrieved catalog of {len(icSrc_table)} sources.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "icSrc_all = pd.concat(icSrc_list, ignore_index=True)\n",
    "\n",
    "print(\"\")\n",
    "print(f\"Total combined catalog contains {len(icSrc_all)} sources.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show resulting pandas dataframe, but only if verbosity level is greater than 0:\n",
    "if verbose > 0:\n",
    "    display(icSrc_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for col in icSrc_all.columns:\n",
    "#    print(col,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2 Save `icSrc_all` as a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if outputCSV:\n",
    "    icSrc_all.to_csv('LSSTComCam_C26202_fields_icSrc.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Calculate psf to total flux aperture magnitudes on a per-visit basis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Match src_all and icSrc_all catalogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will perform the match using the cartesianXYMatchWithError function defined above.\n",
    "\n",
    "df1 = src_all.copy(deep=True)\n",
    "xcol1 = 'x'\n",
    "ycol1 = 'y'\n",
    "\n",
    "df2 = icSrc_all.copy(deep=True)\n",
    "xcol2 = 'base_SdssCentroid_x'\n",
    "ycol2 = 'base_SdssCentroid_y'\n",
    "\n",
    "sep_limit = 1.0\n",
    "\n",
    "df_match = cartesianXYMatchWithError(df1, xcol1, ycol1, df2, xcol2, ycol2, 1.0, False)\n",
    "\n",
    "# Print number of matches\n",
    "print(f\"Number of matches found: {len(df_match)}\")\n",
    "\n",
    "# Print statistics of match distances\n",
    "print(\"\\nMatch distance statistics:\")\n",
    "print(df_match['separation'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show resulting pandas dataframe, but only if verbosity level is greater than 0:\n",
    "if verbose > 0:\n",
    "    display(df_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Create dataframe containing the visit-by-visit median psf-to-total flux aperture corrections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column containing the psf-to-total flux aperture correction for each individual source.\n",
    "\n",
    "# There was a major change in the DRP pipeline starting with w_2025_05.\n",
    "# See:  https://rubin-obs.slack.com/archives/C07TXQUAXUZ/p1738795935921129\n",
    "if post_w_2025_04:\n",
    "    #  We will use 'base_PsfFlux_instFlux' as our primary instrumental flux measurement.\n",
    "    #  We will take 1.04*'base_CircularApertureFlux_12_0_instFlux' as the total flux.\n",
    "    df_match['apCorrTot'] = 1.04*df_match['base_CircularApertureFlux_12_0_instFlux'] / df_match['base_PsfFlux_instFlux']\n",
    "else:\n",
    "    #  We will use 'base_PsfFlux_instFlux' as our primary instrumental flux measurement.\n",
    "    #  We will take 'base_CircularApertureFlux_70_0_instFlux' as the total flux.\n",
    "    df_match['apCorrTot'] = df_match['base_CircularApertureFlux_70_0_instFlux'] / df_match['base_PsfFlux_instFlux']\n",
    "\n",
    "\n",
    "# Create a mask to cull sources with \"bad\" measurements.\n",
    "\n",
    "# There was a major change in the DRP pipeline starting with w_2025_05.\n",
    "# See:  https://rubin-obs.slack.com/archives/C07TXQUAXUZ/p1738795935921129\n",
    "if post_w_2025_04:\n",
    "    mask1 = (~df_match.pixelFlags_bad) & (~df_match.pixelFlags_saturated) & \\\n",
    "            (~df_match.extendedness_flag) & (df_match.detect_isPrimary) & \\\n",
    "            (~df_match.base_CircularApertureFlux_12_0_flag) & \\\n",
    "            (~df_match.base_PsfFlux_flag)  \n",
    "else:\n",
    "    mask1 = (~df_match.pixelFlags_bad) & (~df_match.pixelFlags_saturated) & \\\n",
    "            (~df_match.extendedness_flag) & (df_match.detect_isPrimary) & \\\n",
    "            (~df_match.base_CircularApertureFlux_70_0_flag) & \\\n",
    "            (~df_match.base_PsfFlux_flag)  \n",
    "\n",
    "# Create an another mask to cull sources that are too faint or (possibly) too bright.\n",
    "psfFlux_min = df_match[mask1]['base_PsfFlux_instFlux'].quantile(0.75)\n",
    "psfFlux_max = df_match[mask1]['base_PsfFlux_instFlux'].quantile(0.95)\n",
    "mask = mask1 & (df_match.base_PsfFlux_instFlux >= psfFlux_min) & (df_match.base_PsfFlux_instFlux < psfFlux_max)\n",
    "\n",
    "# Calculate median ratio per visit, ignoring NaNs\n",
    "median_apCorrTots = df_match[mask].groupby('visit')['apCorrTot'].agg(lambda x: np.nanmedian(x))\n",
    "\n",
    "# Create a pandas DataFrame out of this pandas Series\n",
    "df_median_apCorrTots = median_apCorrTots.reset_index()\n",
    "\n",
    "# Rename `apCorrTot` to `apCorrTot_median` in df_median_apCorrTots\n",
    "df_median_apCorrTots.rename(columns={'apCorrTot': 'apCorrTot_median'}, inplace=True)\n",
    "\n",
    "## Remove the original apCorrTot column from df_match\n",
    "#df_match.drop('apCorrTot', axis=1, inplace=True)\n",
    "\n",
    "# Show the dataframe of median apCorrTots by visit id, \n",
    "#  but only if verbosity level is greater than 1:\n",
    "if verbose > 1:\n",
    "    display(df_median_apCorrTots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Add the visit-by-visit median aperture corrections to the `df_match` (combined `src_all`+`icSrc_all`) pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_match = df_match.merge(df_median_apCorrTots, on='visit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display result sorted in ascending order of visit (primarily) and RA (secondarily), \n",
    "#  but only if verbosity level is greater than 0:\n",
    "if verbose > 0:\n",
    "    display(df_match.sort_values(by=['visit', 'ra']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Extract the rows containing C26202 from the matched src_all and icSrc_all catalogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on code retrieved from Claude-3.5-Sonnet\n",
    "\n",
    "# Create a mask to cull sources with \"bad\" measurements.\n",
    "\n",
    "# There was a major change in the DRP pipeline starting with w_2025_05.\n",
    "# See:  https://rubin-obs.slack.com/archives/C07TXQUAXUZ/p1738795935921129\n",
    "if post_w_2025_04:\n",
    "    mask1 = (~df_match.pixelFlags_bad) & (~df_match.pixelFlags_saturated) & \\\n",
    "            (~df_match.extendedness_flag) & (df_match.detect_isPrimary) & \\\n",
    "            (~df_match.base_CircularApertureFlux_12_0_flag) & \\\n",
    "            (~df_match.base_PsfFlux_flag)  \n",
    "else:\n",
    "    mask1 = (~df_match.pixelFlags_bad) & (~df_match.pixelFlags_saturated) & \\\n",
    "            (~df_match.extendedness_flag) & (df_match.detect_isPrimary) & \\\n",
    "            (~df_match.base_CircularApertureFlux_70_0_flag) & \\\n",
    "            (~df_match.base_PsfFlux_flag)  \n",
    "\n",
    "# Apply mask, keeping only the \"good\" measurements of `df_match`\n",
    "df_match_cleaned = df_match[mask1]\n",
    "\n",
    "# Create SkyCoord object for the coordinates of C26202\n",
    "ref_coord = SkyCoord(ra=raDeg*u.degree, dec=decDeg*u.degree)\n",
    "\n",
    "# Create SkyCoord object for all points in the dataframe\n",
    "df_coords = SkyCoord(ra=df_match_cleaned['ra'].values*u.degree, \n",
    "                     dec=df_match_cleaned['dec'].values*u.degree)\n",
    "\n",
    "# Calculate separations\n",
    "separations = ref_coord.separation(df_coords)\n",
    "\n",
    "# Create mask for points within 3.0 arcseconds\n",
    "mask_sep = separations < 3.0*u.arcsec\n",
    "\n",
    "# Get filtered dataframe\n",
    "nearby_good_df = df_match_cleaned[mask_sep]\n",
    "\n",
    "# If you want to include the separations in the result\n",
    "orig_columns = nearby_good_df.columns\n",
    "nearby_good_df = df_match_cleaned[mask_sep].copy()\n",
    "nearby_good_df['separation_c26202'] = separations[mask_sep].arcsec\n",
    "\n",
    "# Find (and keep) the closet match within the match radius\n",
    "best_df = nearby_good_df.sort_values('separation_c26202').drop_duplicates(subset=orig_columns, keep='first')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the resulting table, but only if verbosity level is greater than 1:\n",
    "if verbose > 1:\n",
    "    display(best_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dispaly only the most relevant columns of the resulting table, \n",
    "# but only if verbosity level is greater than 0:\n",
    "if verbose > 0:\n",
    "    display(best_df[['visit', 'band', 'airmass', 'base_PsfFlux_instFlux', 'apCorrTot_median']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for colname in best_df.columns:\n",
    "#    print(colname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Calculate the ratio of observed to expected throughputs for ComCam based on C26202\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Add a column to `best_df` containing the expected counts for C26202 based on the contents of `df_counts` created earlier\n",
    "\n",
    "We will use the `interp1d` interpolation function from the `scipy.interpolate` package to perform linear interpolations between the airmasses listed in `df_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on code retrieved from Claude-3.5-Sonnet\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Create a dictionary to store interpolation functions for each band\n",
    "interpolators = {}\n",
    "for band in ['u', 'g', 'r', 'i', 'z', 'y']:\n",
    "    interpolators[band] = interp1d(df_counts.index, \n",
    "                                 df_counts[band], \n",
    "                                 kind='linear',\n",
    "                                 bounds_error=False,    # Return nan for out of bounds\n",
    "                                 fill_value=np.nan)\n",
    "\n",
    "# Create new column with interpolated values\n",
    "best_df['total_counts_expected'] = best_df.apply(\n",
    "    lambda row: interpolators[row['band']](row['airmass']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# You can check the results if verbosity level is greater than 0):\n",
    "if verbose > 0:\n",
    "    display(best_df[['visit', 'band', 'airmass', 'total_counts_expected']])\n",
    "\n",
    "# Optional: Check for any NaN values (would indicate airmass outside interpolation range)\n",
    "nan_matches = best_df[best_df['total_counts_expected'].isna()]\n",
    "if len(nan_matches) > 0:\n",
    "    print(\"\\nRows with no matches (airmass out of range):\")\n",
    "    print(nan_matches[['visit', 'band', 'airmass']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Add a column to `best_df` containing the total counts observed and the ratio of total counts observed to total counts expected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_df['total_counts_observed'] = best_df['apCorrTot_median'] * best_df['base_PsfFlux_instFlux']\n",
    "best_df['ratio_obs_exp'] = best_df['total_counts_observed'] / best_df['total_counts_expected']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at them..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas to show all rows (but only if verbosity level is greater than 1)...\n",
    "if verbose > 1:\n",
    "    pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to screen the most relevant columns for all rows, \n",
    "#  but only if verbosity level is greater than 0...\n",
    "if verbose > 0:\n",
    "    display(best_df[['visit', 'band', 'airmass', 'base_PsfFlux_instFlux', 'apCorrTot_median', 'total_counts_observed', 'total_counts_expected', 'ratio_obs_exp']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset pandas to its default maximum rows to print to screen\n",
    "# (if it had been reset earlier due to verbosity level greater than 1)...\n",
    "if verbose > 1:\n",
    "    pd.reset_option(\"display.max_rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T00:32:08.865742Z",
     "iopub.status.busy": "2024-12-12T00:32:08.865408Z",
     "iopub.status.idle": "2024-12-12T00:32:08.867982Z",
     "shell.execute_reply": "2024-12-12T00:32:08.867498Z",
     "shell.execute_reply.started": "2024-12-12T00:32:08.865727Z"
    }
   },
   "source": [
    "### 7.3 Plot a histogram of the ratio of total counts observed to total counts for each passband"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on code retrieved from Claude-3.5-Sonnet and Poe.com Assistant\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Define colors and transparency for each band\n",
    "colors = plot_filter_colors_white_background\n",
    "alpha = 1.0   # transparency level\n",
    "linewidth = 3 # linewidth for the step histogram lines \n",
    "\n",
    "# Define bins.  Here, we want to look around ratio=1.00+/-0.20 in steps of 0.01\n",
    "#bins = np.arange(0.80, 1.20, 0.01)\n",
    "bins = np.arange(0.00, 2.00, 0.01)\n",
    "\n",
    "# Plot histogram for each band\n",
    "for band in ['u', 'g', 'r', 'i', 'z', 'y']:\n",
    "    band_data = best_df[best_df['band'] == band]['ratio_obs_exp']\n",
    "    if len(band_data) > 0:  # only plot if we have data for this band\n",
    "        plt.hist(band_data, bins=bins, alpha=alpha, histtype='step', linewidth=linewidth, \n",
    "                label=f'band {band}', color=colors[band],\n",
    "                density=False)  # density=True normalizes the area\n",
    "\n",
    "plt.xlabel('Ratio (Observed Counts/Expected Counts)')\n",
    "plt.ylabel('Number')\n",
    "#plt.xlim([0.80, 1.20])\n",
    "plt.xlim([0.00, 2.00])\n",
    "\n",
    "plt.title('Distribution of Observed/Expected Total Counts Ratio by Band for C26202')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Optional: adjust layout to prevent label clipping\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Print summary statistics for each band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on code retrieved from Claude-3.5-Sonnet\n",
    "\n",
    "for band in ['u', 'g', 'r', 'i', 'z', 'y']:\n",
    "    band_data = best_df[best_df['band'] == band]['ratio_obs_exp']\n",
    "    if len(band_data) > 0:\n",
    "        print(f\"\\nBand {band} statistics:\")\n",
    "        print(f\"N = {len(band_data)}\")\n",
    "        print(f\"Mean = {band_data.mean():.3f}\")\n",
    "        print(f\"Median = {band_data.median():.3f}\")\n",
    "        print(f\"Std = {band_data.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's stop here for now:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Stopping here...\")\n",
    "raise StopExecution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check header of an ISR-corrected exposure image\n",
    "icExp = butler.get('icExp', dataId=dataId)\n",
    "icExp_info = icExp.getInfo()\n",
    "print(icExp_info.getMetadata())\n",
    "print(dataId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Lynne Jones...\n",
    "\n",
    "# Read the file \n",
    "seddata = fits.getdata(sedfile)\n",
    "# Translate to rubin_sim format\n",
    "wavelen = seddata['WAVELENGTH'] * u.angstrom.to(u.nanometer) # This is in angstroms - need in nanometers\n",
    "flambda = seddata['FLUX'] / (u.angstrom.to(u.nanometer)) # this is in erg/sec/cm^^2/ang but we want /nm \n",
    "\n",
    "defaultDirs = st.setDefaultDirs()\n",
    "defaultDirs['detector'] = defaultDirs['detector'].replace('/joint_minimum', '/itl')\n",
    "hardware, system = st.buildHardwareAndSystem(defaultDirs)\n",
    "\n",
    "sed = pt.Sed(wavelen=wavelen, flambda=flambda)\n",
    "\n",
    "\n",
    "# exposure time and gain -- gain=1 -> e- counts\n",
    "phot_params = pt.PhotometricParameters(exptime=30, gain=1, nexp=1)\n",
    "\n",
    "mags = {}\n",
    "counts = {}\n",
    "counts_100k = {}\n",
    "for f in 'ugrizy':\n",
    "    mags[f] = sed.calc_mag(system[f])\n",
    "    counts[f] = sed.calc_adu(system[f], phot_params)\n",
    "    counts_100k[f] = counts[f]/100000\n",
    "\n",
    "#pd.DataFrame([mags, counts, counts_100k], index=['AB mag', 'total counts', 'counts(100k)'])\n",
    "for f in 'ugrizy':\n",
    "    print(f, mags[f], counts[f], counts_100k[f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fgcm_stars = butler.get(\"fgcm_Cycle5_StandardStars\", collections=[\"u/erykoff/LSSTComCam/DM-47919/highlat/build4/run3\"]).asAstropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fgcm_stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fgcm_stars.write('fgcm_stars.ecsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from astropy.table import Table\n",
    "#df_bad_visits=Table.read(\"https://raw.githubusercontent.com/lsst-dm/excluded_visits/refs/heads/main/LSSTComCam/bad.ecsv\").to_pandas()\n",
    "#df_bad_visits.rename(columns={'exposure': 'visit'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_bad_visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_df[best_df['band'] == band]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for colname in best_df.columns:\n",
    "#    print(colname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_df['detector'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_df['id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_df[best_df['band']=='u'].plot('visit','ratio_obs_exp', kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on code retrieved from Claude-3.5-Sonnet and Poe.com Assistant\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Define colors and transparency for each band\n",
    "colors = plot_filter_colors_white_background\n",
    "alpha = 1.0   # transparency level\n",
    "\n",
    "# Plot for each band\n",
    "for band in ['u', 'g', 'r', 'i', 'z', 'y']:\n",
    "    band_data = best_df[best_df['band'] == band]\n",
    "    plt.scatter(band_data['visit'], band_data['ratio_obs_exp'], \n",
    "                label=f'band {band}', \n",
    "                color=plot_filter_colors_white_background[band], \n",
    "                alpha=0.7)\n",
    "    \n",
    "plt.xlabel('Visit')\n",
    "plt.ylabel('Ratio (Observed Counts/Expected Counts)')\n",
    "\n",
    "plt.title('Observed/Expected Total Counts Ratio vs. Visit for C26202')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Optional: adjust layout to prevent label clipping\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on code retrieved from Claude-3.5-Sonnet and Poe.com Assistant\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Define colors and transparency for each band\n",
    "colors = plot_filter_colors_white_background\n",
    "alpha = 1.0   # transparency level\n",
    "\n",
    "# Plot for each band\n",
    "for band in ['u']:\n",
    "    band_data = best_df[best_df['band'] == band]\n",
    "    plt.scatter(band_data['detector'], band_data['ratio_obs_exp'], \n",
    "                label=f'band {band}', \n",
    "                color=plot_filter_colors_white_background[band], \n",
    "                alpha=0.7)\n",
    "    \n",
    "plt.xlabel('detector')\n",
    "plt.ylabel('Ratio (Observed Counts/Expected Counts)')\n",
    "\n",
    "plt.title('Observed/Expected Total Counts Ratio vs. Visit for C26202')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Optional: adjust layout to prevent label clipping\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on code retrieved from Claude-3.5-Sonnet and Poe.com Assistant\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Define colors and transparency for each band\n",
    "colors = plot_filter_colors_white_background\n",
    "alpha = 1.0   # transparency level\n",
    "\n",
    "# Plot for each band\n",
    "for band in ['u']:\n",
    "    band_data = best_df[best_df['band'] == band]\n",
    "    plt.scatter(band_data['airmass'], band_data['ratio_obs_exp'], \n",
    "                label=f'band {band}', \n",
    "                color=plot_filter_colors_white_background[band], \n",
    "                alpha=0.7)\n",
    "    \n",
    "plt.xlabel('Airmass')\n",
    "plt.ylabel('Ratio (Observed Counts/Expected Counts)')\n",
    "\n",
    "plt.title('Observed/Expected Total Counts Ratio vs. Visit for C26202')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Optional: adjust layout to prevent label clipping\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_df['visit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(best_df, df_sci, left_on='visit', right_on='id', how='left')\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on code retrieved from Claude-3.5-Sonnet and Poe.com Assistant\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Define colors and transparency for each band\n",
    "colors = plot_filter_colors_white_background\n",
    "alpha = 1.0   # transparency level\n",
    "\n",
    "# Plot for each band\n",
    "for band in ['u']:\n",
    "    band_data = merged_df[merged_df['band'] == band]\n",
    "    plt.scatter(band_data['airmass_y'], band_data['ratio_obs_exp'], \n",
    "                label=f'band {band}', \n",
    "                color=plot_filter_colors_white_background[band], \n",
    "                alpha=0.7)\n",
    "    \n",
    "plt.xlabel('Airmass')\n",
    "plt.ylabel('Ratio (Observed Counts/Expected Counts)')\n",
    "\n",
    "plt.title('Observed/Expected Total Counts Ratio vs. Visit for C26202')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Optional: adjust layout to prevent label clipping\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on code retrieved from Claude-3.5-Sonnet and Poe.com Assistant\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Define colors and transparency for each band\n",
    "colors = plot_filter_colors_white_background\n",
    "alpha = 1.0   # transparency level\n",
    "\n",
    "# Plot for each band\n",
    "for band in ['u', 'g', 'r', 'i', 'z', 'y']:\n",
    "    band_data = merged_df[merged_df['band'] == band]\n",
    "    plt.scatter(band_data['mjd'], band_data['ratio_obs_exp'], \n",
    "                label=f'band {band}', \n",
    "                color=plot_filter_colors_white_background[band], \n",
    "                alpha=0.7)\n",
    "    \n",
    "plt.xlabel('MJD')\n",
    "plt.ylabel('Ratio (Observed Counts/Expected Counts)')\n",
    "\n",
    "plt.title('Observed/Expected Total Counts Ratio vs. Visit for C26202')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Optional: adjust layout to prevent label clipping\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on code retrieved from Claude-3.5-Sonnet and Poe.com Assistant\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Define colors and transparency for each band\n",
    "colors = plot_filter_colors_white_background\n",
    "alpha = 1.0   # transparency level\n",
    "\n",
    "# Plot for each band\n",
    "#for band in ['u', 'g', 'r', 'i', 'z', 'y']:\n",
    "for band in ['u']:\n",
    "    band_data = merged_df[merged_df['band'] == band]\n",
    "    plt.scatter(band_data['expos'], band_data['ratio_obs_exp'], \n",
    "                label=f'band {band}', \n",
    "                color=plot_filter_colors_white_background[band], \n",
    "                alpha=0.7)\n",
    "    \n",
    "plt.xlabel('exposure time [sec]')\n",
    "plt.ylabel('Ratio (Observed Counts/Expected Counts)')\n",
    "\n",
    "plt.title('Observed/Expected Total Counts Ratio vs. Visit for C26202')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Optional: adjust layout to prevent label clipping\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
